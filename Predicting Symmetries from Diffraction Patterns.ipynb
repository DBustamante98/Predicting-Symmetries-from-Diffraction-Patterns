{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0f828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mp_api.client import MPRester\n",
    "import numpy as np\n",
    "from mendeleev import element\n",
    "import re\n",
    "import inspect \n",
    "from IPython.utils import io\n",
    "from monty.serialization import dumpfn, loadfn\n",
    "import time\n",
    "import gzip\n",
    "import json\n",
    "from PIL import Image \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update(\n",
    "    {\n",
    "        'text.usetex': False,\n",
    "        'font.family': 'stixgeneral',\n",
    "        'mathtext.fontset': 'stix',\n",
    "    })\n",
    "import cv2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import heapq\n",
    "import dtreeviz\n",
    "\n",
    "from pymatgen.analysis.diffraction.xrd import XRDCalculator\n",
    "from pymatgen.symmetry.analyzer import SpacegroupAnalyzer\n",
    "from pymatgen.analysis.diffraction.tem import TEMCalculator\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import schedules\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor, RandomForestClassifier\n",
    "from sklearn.tree import plot_tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a654c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chemical_elements = ['H', 'He', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Ne', 'Na', 'Mg', 'Al', 'Si', 'P', 'S', 'Cl', 'Ar', 'K', 'Ca', 'Sc', 'Ti', 'V', 'Cr', 'Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Zn', 'Ga', 'Ge', 'As', 'Se', 'Br', 'Kr', 'Rb', 'Sr', 'Y', 'Zr', 'Nb', 'Mo', 'Tc', 'Ru', 'Rh', 'Pd', 'Ag', 'Cd', 'In', 'Sn', 'Sb', 'Te', 'I', 'Xe', 'Cs', 'Ba', 'La', 'Ce', 'Pr', 'Nd', 'Pm', 'Sm', 'Eu', 'Gd', 'Tb', 'Dy', 'Ho', 'Er', 'Tm', 'Yb', 'Lu', 'Hf', 'Ta', 'W', 'Re', 'Os', 'Ir', 'Pt', 'Au', 'Hg', 'Tl', 'Pb', 'Bi', 'Po', 'At', 'Rn', 'Fr', 'Ra', 'Ac', 'Th', 'Pa', 'U', 'Np', 'Pu', 'Am', 'Cm', 'Bk', 'Cf', 'Es', 'Fm', 'Md', 'No', 'Lr', 'Rf', 'Db', 'Sg', 'Bh', 'Hs', 'Mt', 'Ds', 'Rg', 'Cn', 'Nh', 'Fl', 'Mc', 'Lv', 'Ts', 'Og']\n",
    "atomic_masses = [element(elem).mass for elem in chemical_elements]\n",
    "\n",
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "        \n",
    "def gzip_load(file):\n",
    "    with gzip.open(file, \"r\") as f:\n",
    "        data = f.read()\n",
    "        data = json.loads(data.decode('utf-8'))  \n",
    "        return [AttrDict(i) for i in data]\n",
    "\n",
    "def get_atom_vector(material, atoms, natoms=True, ndensity=True):\n",
    "    atom_vector = np.zeros(len(chemical_elements))\n",
    "    atoms_ = re.findall(r'[A-Z][a-z]*|\\d+', re.sub('[A-Z][a-z]*(?![\\da-z])', r'\\g<0>1', atoms))\n",
    "    if natoms:\n",
    "        for i in range( len(atoms_)//2 ): atom_vector[ chemical_elements.index(atoms_[2*i]) ] += int( atoms_[2*i+1]  )\n",
    "    else:\n",
    "        for i in range( len(atoms_)//2 ): atom_vector[ chemical_elements.index(atoms_[2*i]) ] += 1\n",
    "    if ndensity: atom_vector = np.append(atom_vector, material.density)\n",
    "    return atom_vector\n",
    "\n",
    "mpr = MPRester(\"YOUR_API\")\n",
    "calculator = XRDCalculator(wavelength='CuKa')\n",
    "TEMcalculator = TEMCalculator(voltage = 200, beam_direction = (0, 0, 1))\n",
    "_p_ = None  \n",
    "try: _p_ = np.load(\"_p_.npy\") # To shuffle the materials\n",
    "except: pass\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeb823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the materials   \n",
    "try:\n",
    "    print(\"Loading materials...\")\n",
    "    start = time.time()\n",
    "    materials = np.load(\"materials.npy\", allow_pickle=True) # loadfn(\"materials.json.gz\")\n",
    "    materials = [dotdict(i) for i in materials]\n",
    "    #for i in materials: i.symmetry = dotdict(i.symmetry)\n",
    "    end = time.time()\n",
    "except:\n",
    "    start = time.time()\n",
    "    print(\"Failed.\\nRetrieving materials from The Materials Project...\")\n",
    "    materials = mpr.summary.search(volume=(0,100000000), fields=[\"material_id\", \"structure\", \"formula_pretty\", \"volume\", \"symmetry\", \"xas\", \"elements\", \"density\", \"density_atomic\"])\n",
    "    \n",
    "    for i in materials:\n",
    "        dictionary = {\"crystal_system\": str(i.symmetry.crystal_system), \"symbol\": i.symmetry.symbol,\n",
    "                 \"number\": i.symmetry.number, \"point_group\": i.symmetry.point_group, \"symprec\": i.symmetry.symprec,\n",
    "                 \"version\": i.symmetry.version}\n",
    "        i.symmetry = dotdict(dictionary)\n",
    "        i.xas = str(i.xas)\n",
    "    #dumpfn(materials, \"materials.json.gz\")\n",
    "    end = time.time()\n",
    "    \n",
    "print(len(materials), \" materials in\", np.round((end - start)/60,2), \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7430ab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the conventional structures\n",
    "try:\n",
    "    print(\"Loading conventional structures...\")\n",
    "    start = time.time()\n",
    "    conventional_structures = np.load(\"conventional_structures.npy\", allow_pickle=True) #  loadfn(\"conventional_structures.json.gz\")\n",
    "    end = time.time()\n",
    "except:\n",
    "    print(\"Failed.\\nGetting conventional structures...\")\n",
    "    start = time.time()\n",
    "    conventional_structures = []\n",
    "    check = 0\n",
    "    for index, material in enumerate(materials):\n",
    "        if index%10==0: print( \"  \"+str(np.round(index/len(materials)*100, 3))+\"%    \"+str(check)+\"    \", end='\\r' ) \n",
    "        try:\n",
    "            sga = SpacegroupAnalyzer(material.structure)\n",
    "            conventional_structure = sga.get_conventional_standard_structure()\n",
    "            conventional_structures.append(conventional_structure)\n",
    "            check += 1\n",
    "        except: conventional_structures.append(material.structure)\n",
    "    dumpfn(conventional_structures, \"conventional_structures.json.gz\")\n",
    "    end = time.time()\n",
    "    \n",
    "print(np.round((end - start)/60,2), \"min.                    \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f78468",
   "metadata": {},
   "outputs": [],
   "source": [
    "space_groups_230 = []\n",
    "with open(\"SpaceGroups.txt\",'r') as data_file:\n",
    "    for line in data_file:\n",
    "        data = (line.replace(\" \",\"\").replace(\"\\n\",\"\")).split(\",\")\n",
    "        for i in data: \n",
    "            if len(i)>0: space_groups_230.append(i)\n",
    "                \n",
    "try:\n",
    "    print(\"Loading patterns...\")\n",
    "    start = time.time()\n",
    "    patterns = np.load(\"Xpatterns.npy\", allow_pickle=True) # loadfn(\"Xpatterns.json.gz\")\n",
    "    patterns = [dotdict({\"x\": pattern[0], \"intensity\": pattern[1]}) for pattern in patterns]\n",
    "    crystal_systems = np.load(\"crystal_systems.npy\", allow_pickle=True)\n",
    "    point_groups = np.load(\"point_groups.npy\", allow_pickle=True)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(np.round((end - start)/60,2), \"min.                    \")\n",
    "except:\n",
    "    print(\"Failed.\\nGetting patterns...\")\n",
    "    patterns = []\n",
    "    crystal_systems = []\n",
    "    point_groups = []\n",
    "    check = 0\n",
    "\n",
    "    for i, conventional_structure in enumerate(conventional_structures):\n",
    "        try:\n",
    "            if i%10==0: print( \"  \"+str(np.round(i/len(conventional_structures)*100,3))+\"%    \"+str(check)+\"      \", end='\\r' )      \n",
    "            pattern = calculator.get_pattern(conventional_structure)\n",
    "            patterns.append(pattern)\n",
    "            crystal_systems.append( str(materials[i].symmetry.crystal_system) )\n",
    "            point_groups.append( materials[i].symmetry.symbol )\n",
    "            check += 1\n",
    "        except: pass\n",
    "    np.save(\"Xpatterns.npy\", patterns)\n",
    "\n",
    "crystal_systems_classes = [*set(crystal_systems)]\n",
    "point_groups_classes = space_groups_230"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e289511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Npeaks = 20 #  Number of peaks to analyze\n",
    "test_train = 0.9 # Proportion of data that will be used for training\n",
    "include_intensities = True # To include the intensity of each peak in the input\n",
    "include_n_peaks = False # To include the number of peaks in the input\n",
    "\n",
    "max_features = 2*Npeaks if include_intensities else Npeaks\n",
    "XRD_data = []; crystal_system_data = []; crystal_system_data_hot = []; point_group_data = [];  point_group_data_hot = []\n",
    "for i, pattern in enumerate(patterns):\n",
    "    new_data_x = 4*np.pi * np.sin( (pattern.x/2)*np.pi/180 )/1.542   # 4*np.pi * np.sin( (pattern.x/2)*np.pi/180 )/1.542  2*np.sin( (pattern.x/2)*np.pi/180 )/1.542   1.542/(2*np.sin( (pattern.x/2)*np.pi/180 ))    (pattern.x)/90              // np.sin( (pattern.x/2) *np.pi/180 )/1.542\n",
    "    new_data_I = pattern.intensity/100\n",
    "    how_many_peaks = len(pattern.intensity)\n",
    "    if len(new_data_x)>=Npeaks:\n",
    "        if include_intensities: XRD_data.append( np.concatenate((new_data_x[0:Npeaks],new_data_I[0:Npeaks])) ) # .reshape(2,Npeaks)\n",
    "        else: XRD_data.append( new_data_x[0:Npeaks] )\n",
    "    else:\n",
    "        new_data_x = np.pad(new_data_x, (0,Npeaks-len(new_data_x)), constant_values=(-1))\n",
    "        new_data_I = np.pad(new_data_I, (0,Npeaks-len(new_data_I)), constant_values=(-1))\n",
    "        if include_intensities: XRD_data.append( np.concatenate((new_data_x,new_data_I)) )   \n",
    "        else: XRD_data.append( new_data_x )\n",
    "    if include_n_peaks:\n",
    "        XRD_data[-1] = np.append(XRD_data[-1], how_many_peaks)\n",
    "        \n",
    "    dvar = 1 if crystal_systems[i]==\"Cubic\" else 0    \n",
    "        \n",
    "    crystal_system_data.append( dvar ) # crystal_systems_classes.index(crystal_systems[i])\n",
    "    hot_vector = np.zeros( len(crystal_systems_classes) );\n",
    "    hot_vector[crystal_system_data[-1]] = 1\n",
    "    crystal_system_data_hot.append(hot_vector)\n",
    "    hot_vector = np.zeros( 230 );\n",
    "    point_group_data.append( space_groups_230.index(point_groups[i].replace(\"Cmce\",\"Cmca\")) )\n",
    "    hot_vector[point_group_data[-1]] = 1\n",
    "    point_group_data_hot.append(hot_vector)\n",
    "XRD_data = np.array(XRD_data); crystal_system_data = np.array(crystal_system_data); point_group_data = np.array(point_group_data)  \n",
    "crystal_system_data_hot = np.array(crystal_system_data_hot); point_group_data_hot = np.array(point_group_data_hot)\n",
    "\n",
    "def unison_shuffled_copies(a, b, c):\n",
    "    global _p_\n",
    "    assert len(a) == len(b) and len(a) == len(c)\n",
    "    if _p_ is not None: return a[_p_], b[_p_], c[_p_]\n",
    "    p = np.random.permutation(len(a))\n",
    "    _p_ = p\n",
    "    return a[p], b[p], c[p]\n",
    "Xdata, Y1data, Y2data = unison_shuffled_copies(XRD_data, crystal_system_data, point_group_data)\n",
    "\n",
    "test_train = int(test_train*len(Xdata))\n",
    "X_train = Xdata[0:test_train]; Y1_train = Y1data[0:test_train]; Y2_train = Y2data[0:test_train]\n",
    "X_test = Xdata[test_train:]; Y1_test = Y1data[test_train:]; Y2_test = Y2data[test_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bba436",
   "metadata": {},
   "source": [
    "# XRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888dd3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ExtraTreesClassifier for crystalline systems\n",
    "random_state = 42\n",
    "get_crystal_system = ExtraTreesClassifier(n_estimators=500, # 600\n",
    "                            max_depth=30, # 40\n",
    "                            max_features=100, # 20\n",
    "                            n_jobs=-1, \n",
    "                            random_state=random_state,\n",
    "                            warm_start=False)\n",
    "get_crystal_system.fit(X_train, Y1_train)\n",
    "\n",
    "y1_pred0 = get_crystal_system.predict(X_train)\n",
    "print('Accuracy of crystal system prediction (train): ', metrics.accuracy_score(Y1_train, y1_pred0)*100, \"%\")\n",
    "y1_pred = get_crystal_system.predict(X_test)\n",
    "print('Accuracy of crystal system prediction (test): ', metrics.accuracy_score(Y1_test, y1_pred)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eefa5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ExtraTreesClassifier for space groups\n",
    "random_state = 24\n",
    "get_point_group = ExtraTreesClassifier(n_estimators=200, # 200\n",
    "                            max_depth=25, # 20\n",
    "                            max_features=100, # max_features\n",
    "                            n_jobs=-1, \n",
    "                            random_state=random_state, # 42\n",
    "                            warm_start=False)\n",
    "get_point_group.n_classes_ = 230\n",
    "get_point_group.fit(X_train, Y2_train)\n",
    "\n",
    "y2_pred0 = get_point_group.predict(X_train)\n",
    "print('Accuracy of point group prediction (train): ', metrics.accuracy_score(Y2_train, y2_pred0)*100, \"%\")\n",
    "y2_pred = get_point_group.predict(X_test)\n",
    "print('Accuracy of point group prediction (test): ', metrics.accuracy_score(Y2_test, y2_pred)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7d43a6",
   "metadata": {},
   "source": [
    "# TEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf237fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMpatterns = []\n",
    "\n",
    "try:\n",
    "    TEMfiles = []\n",
    "    for (_, _, filenames) in os.walk(\"./TEMpatterns\"):\n",
    "        for filename in filenames:\n",
    "            TEMpatterns.append( 255-cv2.imread(\"./TEMpatterns/\"+filename, cv2.IMREAD_GRAYSCALE) )\n",
    "            TEMfiles.append(int(filename.split(\".png\")[0]))\n",
    "    TEMpatterns = np.array( [x for _, x in sorted(zip(TEMfiles, TEMpatterns))] ) \n",
    "except:\n",
    "    for i, conventional_structure in enumerate(conventional_structures[0:100]):\n",
    "        print( \"  \"+str(np.round(i/len(conventional_structures[0:100])*100,2))+\"%  \", end='\\r' ) \n",
    "        TEMpattern = TEMcalculator.get_plot_2d(conventional_structure)\n",
    "        file_name = \"TEMpatterns/\"+str(i)+\".png\"\n",
    "        TEMpattern.write_image(file_name)\n",
    "        im_gray = (255-cv2.imread(file_name, cv2.IMREAD_GRAYSCALE))[100:470, 90:460]\n",
    "        TEMpatterns.append(im_gray)\n",
    "        cv2.imwrite(file_name, im_gray)\n",
    "    TEMpatterns = np.array(TEMpatterns)/255    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb786b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pTEM = np.random.permutation(len(TEMpatterns))\n",
    "TEMpatterns, Ydata_TEM = TEMpatterns[pTEM], crystal_system_data_hot[0:len(TEMpatterns)][pTEM]\n",
    "TEMpatterns = np.expand_dims(TEMpatterns, axis=-1)\n",
    "TEM_train_test = 0.1\n",
    "NTEM = int(TEM_train_test*len(TEMpatterns))\n",
    "input_shape = TEMpatterns.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4a9a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEM_train_test = 0.1\n",
    "NTEM = int(TEM_train_test*len(TEMpatterns))\n",
    "Xtrain_TEM, Ytrain_TEM = TEMpatterns[0:8*NTEM], Ydata_TEM[0:8*NTEM]\n",
    "Xvalidation_TEM, Yvalidation_TEM = TEMpatterns[8*NTEM:9*NTEM], Ydata_TEM[8*NTEM:9*NTEM]\n",
    "Xtest_TEM, Ytest_TEM = TEMpatterns[9*NTEM:], Ydata_TEM[9*NTEM:]\n",
    "input_shape = TEMpatterns.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c131c3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "def create_CNN():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(10, kernel_size=(5, 5),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(20, (5, 5), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(20*4*4, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(len(crystal_systems_classes), activation='softmax'))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer='Adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1792ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=3, restore_best_weights=True)\n",
    "model_CNN=create_CNN()\n",
    "model_CNN.fit(TEMpatterns[0:8*NTEM], Ydata_TEM[0:8*NTEM],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          callbacks=[stopping_callback],\n",
    "          validation_data=(TEMpatterns[8*NTEM:9*NTEM], Ydata_TEM[8*NTEM:9*NTEM]))\n",
    "\n",
    "score = model_CNN.evaluate(TEMpatterns[9*NTEM:], Ydata_TEM[9*NTEM:], verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
